.. Kenneth Lee 版权所有 2023

:Authors: Kenneth Lee
:Version: 0.1
:Date: 2023-12-19
:Status: Draft

对卷积的感性理解
****************

最近做一个机器学习相关的内存性能优化时，对卷积这种运算，多了不少感性的了解，我
把这种“感性”的认识总结一下。

卷积本质是一种“权重”计算。比如我们看到一个事物，它有3个特征，我们总结它具有有
个特性的性质，那么一种最直接的认识就是一个这样的计算：::

  [f1, f2, f3] * [w1,
                  w2,
                  w3] = f1*w1 + f2*w2 + f3*w3

你看，我们看到到三个特征(f1, f2, f3)，给它3个权重(w1, w2, w3)，我们最这个东西
的总结就是f1*w1+f2*w2+f3*w3。这符合我们一般理解世界的方式：这个学生语文能力8分，
数学能力6分，体育能力10分。数学最重要，权重5，语文3，体育2。这就构成我们对这个
学生的理解：他是个综合能力7.4分的学生。这包含偏见，但大部分时候，我们就是这样
理解世界的。

这种方法有两个缺点，一个是非线性化，这通常可以通过在综合分数乘上一个非线性函数
（比如softmax），让它不要那么“生硬”，让结果相对“平滑”，这样容易找到公式去微调
这些权重，从而根据训练的结果更容易去修改权重。不过这个问题和我们讨论卷积的问题
无关，我们在本文中不讨论。

另一个问题就是这个依据比较单一，如果输入的特征有多个相关性，在A情况下f1权重比
较高，在B情况下，f2的权重比较高。这样你固定就只调整这一种权重，这样理解问题就
比较刻板。想象一下，我们还是考虑前面说的三个学生成绩，但在校运会和奥数比赛两个
表现中，人们对这个学生的“综合评价”不一样。如果你只能设置一个权重，最多就是这次
调高一点数学，下次调高一点体育，这个判断就怎么都不能贴合这个学生的“综合素质”。

所以，我们要让我们对这些指标的判断更加“立体”一点，不能只有一个权重，比如，我们
把f1的权重设置成两个，这就是一个多维的矩阵乘法了：::

  [f1, f2, f3] * [[w11, w12],
                  [w21, w22],
                  [w31, w32] = [f1*w11+f2*w21+f3*w31, f1*w12+f2*w22+f3*w33]

也就是说，我们用了两个可以在不同的情况下做不同评价的w11, W12两个权重去评价f1了。
矩阵乘法综合可以压扁中间一维，所以它很容易组合起来：::

  1x3 * 3x4 * 4x3 * 3x100 * 100x1 = 1x1

两个矩阵相乘，中间的一层总是被压平，这样我们就可以任意安排这些层，最终把一个简
单的输入，展开到一个综合的评价体系中，这样我们可以通过不同在一次次的“经验”（比
如学生不同的成绩表现得到不同的评价），获得一个最接近的参数集，从而就更容易“预
期”他的成绩和得到的综合评价的相关性了。当然，如果只有这三个参数，本来就没什么
规律，很难做出这种判断。但如果输入的是非常多的要素，可能相关性就变得强了。比如
从一幅图的点阵判断这幅图上画了什么（人脸识别等）。

所以卷积神经网络把两头的纬度叫“显层”（比如上面例子的1x3举证和最后的100x1矩阵），
而把中间的卷积层叫“隐层”。显层就是我们眼前看见的东西，做出的判断，比如看见一个
老虎，得到结论：“快跑”，脑子里面怎么这么快做出这个判断的，说不清楚，这就是一个
“隐层”，里面就可以理解为这种近似的卷积运算，这种运算的中间结果有什么“理智”的解
释？其实也是说不清楚的。

这个理解同样可以用于LLM（大语言模型）。LLM来自翻译技术，比如“This is Apple”，
如果一个个单词翻译，最简单的对照方法就是：::

  This  -> 这
  is    -> 是
  Apple -> 苹果

但这个上下文中，Apple大写了，而且前面没有an，这更应该翻译为“苹果公司”。所以，
这里关键是解决这个“上下文”的问题，LLM的现在发展的主要技术叫attension（关注），
主要技术就是把整个句子前后的文字都和当前翻译的这个单词建立一个不同权重的关联，
这样这些文字都成了翻译这个单词的一个“关注度”，这个算法会用每个上下文来计算所有
单词的QKV(Query-Key-Value），得到一个“1x单词表大小”的显层，然后后面挂上几十层
不同大小的隐层，最终吐出这个词的翻译结果。

这样各种机器学习问题，都变成你怎么建立一个模型，让关注的要素都能灌到一个神经网
络中，然后不断训练它改变权重的过程了。

.. Kenneth Lee 版权所有 2024

:Authors: Kenneth Lee
:Version: 0.1
:Date: 2024-07-23
:Status: Draft

LKMM
****

介绍
====

本文解释一下LKMM模型的原理。我在我的私有笔记上对这个做了较长时间的调研了，但这
个领域太多变化，我甚至觉得不一定有一个统一的认识，所以，到现在我还有不少认识和
LKMM的文档的说明不完全一致，但已经不多了，所以我想写这个公开笔记上总结，把这些
认识整合一下，以便我自己对我们指令集的进一步定义打一个概念空间的地基，同时也把
这些认识分享给更多的中文世界的开发者。

内存模型和内存序模型简介
------------------------

LKMM，Linux Kernel Memory Model，Linux内存模型，是一个对Linux内部API在内存上的
访问模式的建模。

内存模型是个很模糊的说法。在我写我们的指令架构手册的时候，我把所有内存访问的行
为特性都叫做内存模型，包括如何寻址，寻址的时候可以指定多大的访问范围，允许有什
么样的边际效应，地址如何翻译，原子性，内存序，Cache的使用，都看作是内存模型的
一部分。

但我也看过很多上下文仅仅把其中一个主题作为内存模型，比如有些资料会把虚拟地址翻
译特性称为内存模型。而我们现在讨论的LKMM，谈到内存模型，则主要指的是内存序模型。

那么什么是内存序模型呢？就是我们怎么认知我们的内存改变和读取的顺序。我们先从单
个CPU说起，比如你写这样的代码（假定内存的初值都是0）：

.. code-block:: c

  *((int *)0x12340000) = 1;
  *((int *)0x12340000) = 2;
  output(*((int *)0x12340000));

最后输出的应该是多少？显然应该是2，不能是1啊。因为我们对内存改变的顺序是有期望
的。总得是先改成1，再改成2，然后才读的，不能是先改成2，然后才读，最后才改成1的。
如果是后面这样，我们的程序就没法编了。

但如果我们把这些行为分解到多个CPU上，又怎么说呢？比如前两个指令放在一个CPU上，
最后一个指令放在另CPU上，这一点还成立吗？或者我们把问题更突出一点，我们把代码
改写成这样：::

       CPU1                             CPU2
  *((int *)0x12340000) = 1;      while (*((int *)0x12340000))!=1);
  *((int *)0x56780000) = 2;      output(*((int *)0x56780000));
  
output应该是多少？肯定是2吗？换句话说，我们承认在单个CPU上指令是有序的，但我们
承认这个顺序被其他CPU观察到的时候，这也是一样的吗？

有人可能认为“显然啊，把内存看作一个实体，这明明就是先修改了0x12340000再修改
0x56780000，CPU当然也应该先看到0x12340000被修改了，再看到0x56780000被修改啊。

这种想法可以用这张图来示意一下：

.. figure:: _static/mem_order.svg

你看我在图标注上用的语气，就能看出来，认为整片巨大的内存，非要等一个CPU修改完
才轮到下一个CPU，这不那么被待见。而且这确实不是一种“显而易见必须有的承诺”。没
人承诺过整片巨大的内存是个单独的实体啊，实际上它就是多片DIMM条组成的，甚至连在
不同的CPU的插槽上，再通过更慢的总线连在一起的。

所以，这种把CPU看作是一个整体排队接受访问请求的想法只能用作一种“学术意义上的基
线模型”，称为SC，Sequential Consistency。其他实用的商业实现（比如我们熟悉的
x86，ARM，PPC等等），几乎都不是这样保证的。

其实我们还可以换一个角度：我们为什么非要在CPU之间保序呢？本来就是CPU的辅助存储，
又不是用来做通讯的，我们能不能把通讯留给通讯，内存留给内存呢？

我曾经做过这方面的方案，让内存只在单个CPU上保序，如果两个CPU之间要通讯，只能通
过独立的“队列”服务来完成。这种方法在特定的场景上其实非常高效，特别是那种AMP信
号处理系统上，比如收到一个通知，向另几个服务提供另一组通知，进而引发更多的通知。
这种就很高效。但如果要发送的不是简单的控制信号，而是大量的数据，这种方法就不实
用了。比如你处理的是个报文系统，我们收到一个报文，先由层二协议处理，然后转给层
三，再转给层四，每层因为计算压力不同，部署在不同的CPU上，每层只是修改同一片内
存的不同位置，这种你很难把整个报文反反复复在不同的CPU之间发送的。到头来你只能
让内存访问是有序的，保证你的通知信号（虽然不必要，但这个信号常常就是内存变化本
身）发到另一个CPU的时候，所需的内存的更新已经到达这个CPU了。

所以这个没有什么办法，即使我们现在也提供了独立于内存CPU间信号通知手段，这种通
知机制还是必须和内存的更新顺序问题结合在一起来用，没法不考虑内存的实际读写顺序。

所以，这里的关键问题在于：内存不是系统中的一个单一的实体，而是很多很多，被很多
CPU共享的公共存储单元组成的。如果我们要求所有的访问者都要排队，那就会拖慢整个
系统计算的速度。这种排队，无论对于直接访问内存，还是通过CC协议在多个CPU之间同
步Cache的状态，都是一样的。


内存序模型
==========

这一章我们分编程、数学、物理三个模型来讨论内存序模型的更多细节。

内存序的编程模型
----------------

内存序模型在编程上主要解决这样一个问题：我在一个上下文中按某个顺序更新了几个内
存，我能否在另一个观察中保证我看到的顺序和这个顺序是一致的。

我们拿MP（Message Passing）模式来具象一下这个意思。考虑如下程序：::

        上下文1                   上下文2
     update_data();            wait_ready_flags();
     set_ready_flags();        use_data();

第一个上下文中，我们更新了数据（data），然后我们把两个上下文的通讯标记（flag）
置位。第二个上下文，我们等待flag被置位，然后我们再去使用data。我们编程上的期望
是：如果我这样写了，我的use_data()一定能用到update_data()所更新的数据。

我们前面说过了，如果我们可以写成这样：::

        上下文1                   上下文2
     push(data);                use_data(pop());

我们是不需要这种保序功能的，这才是我们原始的诉求，但我们前面也说过了，这种方法
效率不高。所以，我们只能对内存的更新顺序有所要求了。

这个问题，就算队列只发送一部分数据，比如只指针发过来，我们还是有保序要求的：::

        上下文1                   上下文2
     update_data();             data_p=pop()
     push(data_p);              use_data(*data_p);

我们就是要跨着两个上下文，让use_data()用到update_data()的数据。保证上下文1的更
新被“传播”到上下文2上。

在上面的讨论中，我们一直只说“上下文”，而不说CPU，因为我们编程的时候不一定有CPU
这个概念的，我们只有线程的概念，线程表示我们承诺了我们的行为是一个“序”，我们用
这个序来谈我们的期望。这个可以是我们某种编程库上的线程（比如pthread）的概念，
也可以是CPU的执行本身，因为CPU也维护了一种序。谈编程期望的时候我们不考虑这具体
是什么，但到实现到具体的上下文中，这还是需要考虑的。

内存序的数学模型
----------------

序这个问题，是有专门的数学理论的，它的基础就是集合论（Set Theory）。在这种理论
中，顺序表达为一种关系的集合。我们看个例子，比如下面这个顺序：::

  a -> b -> c -> d

从信息论上，我们的结论是a先于b，b先于c，c先于d。这样我们可以描述这个集合R：::

  R = {(a, b), (b, c), (c, d)}

但，在上面那个顺序中，我们是否还有“a先于c”这个信息？细想想，确实是有的，这也是
我们的信息的一部分。所以我们把这部分信息补充一下，上面这个集合应该这样写：::

  Q = {(a, b), (a, c), (a, d), (b, c), (c, d), (c, d)}

我们把可以用来组成关系的元素的集合称为E，上面这个例子中，E={a, b, c, d}。

这样，我们可以这样定义Q：Q是R的超级，对于任意元素x、y、z，x、y、z属于E，如果(x,
y)，（y, z)属于R，那么(x, z)属于Q。

Q称为R的迁移闭包。用后面我们会谈到的cat语言，这可以标记为：::

  Q = R+

有了一个这样的基础定义，我们就可以用集合的方法来对我们定义的各种序来进行数学运
算了。这就构成了一个数学模型，让我们可以研究各种序的组合关系。比如前面的MP问题，
我们这样定义这个问题：::

  令：
  up = update_data()
  s = set_ready_flags()
  w = wait_ready_flags()
  us = use_data();

  已知：
  上下文1指定的顺序：{(up, s)}
  上下文2指定的顺序：{(w, us)}

  问：需要增加什么条件才能保证：
  {(up, us)}总是成立？

这样就变成一个数学问题了。我们用集合论（其实还包括一些一阶谓词逻辑的理论）来研
究这个问题。

我这里的讨论不一定需要读者去深入学习集合论和一阶谓词逻辑（但看一些基本的内容是
有好处的），我这里先做一些基本的科普以便读者可以看懂后面的内容。

首先，我提醒读者注意：有了上面这样的定义后，我们一般理解的“序”就变成了一种特殊
的概念了，因为我们一般定义的一个序，不是这个泛泛的关系组合，我们还要求它无环
（不能a先于b，b先于c，c又先于a），可迁移，任意两者可比。这样的要求对比我们前面
关于“关系”的定义，其实约束已经非常多了。

在数学上，满足所有这些约束的，我们称为全序（Total Order）。如果仅仅是无环，可
迁移，而不是任意两者可比，这种我们称为偏序。如果能保证无环，那我们还可以认为这
是一个序，至少可比的时候还有先后的特征，一旦有了环，就无法确定谁在谁的前面了。
这种情况，我们就只能认为这是一种泛泛的“关系”，而无法把它称为“序”了。

为了方便，如果两个对象a, b可比，a先于b，我们会记做“a>b”。

内存序问题的关键期望，甚至不是一个序。我们只是要求所有行为的其中一部分，有一定
的先后关系。或者我们可以这样说：我们只是要求部分事件的子集，是一个序。

更多需要的数学基础概念，我们介绍概念的时候顺带描述，以支持读者可以不需要翻太多
的数学书就可以阅读下去。

数学建模工具
~~~~~~~~~~~~

你可以想象得到，推理“序”这种数学关系很难不使用计算机配合的，因为这种基于“集合”
而不是公式的推理，不穷举几乎没法做到。所以，我们的介绍很难不和数学工具结合在一
起。内存序这个领域，早期都是用一些通过的建模工具来做，比如之前分析过的
:doc:`Sail`\ 。还有更多的人使用自己开发的专用工具，现在慢慢都在统一到Herd（本
文写作的时候Herd的最新版本是7，它的语法在不同版本见是有更改的，所以我们确切一
点，我们按一般习惯称为Herd7）上了。

Herd使用一个类似Ocaml（Herd自己就是用Ocaml写的）的语法定义上面提到的集合论的运
算，这种定义文件用.cat作为扩展名，所以一般把这种定义的格式称为cat格式。现在很
多流行的平台，比如x86，ARM等都在使用cat格式，RISCV原来使用Sail，现在也切换到
cat上了，我们要讨论的LKMM现在也是用cat格式定义的。

Herd7的主页在这里：\ `herdtool7 <http://diy.inria.fr>`_\ 。上面有手册（但不是非常完善，很
多东西都没有深入解释，这里还有一个其他人写的总结：
`herd <https://mirrors.edge.kernel.org/pub/linux/kernel/people/paulmck/LWNLinuxMM/herd.html>`_
，可以参考。）我这里不打算介绍它的使用细节，我主要解释一下它的原理和基本
思路和概念。我自己第一次研究它的时候浪费了很多时间在这些基本思路和概念的理解上，
希望我这个介绍可以让读者避免走一样的弯路。

首先我们要理解，Herd7本身是个穷举功能。它的原理是你定义一个cat文件，说明所有的
约束，然后你再说明一个场景（比如像前面这种MP，这称为一个Litmus测试），它帮你穷
举你期望的某个条件是否可以成立。它不能给你完整的“定义”，不能确定你的定义满足某
个要求，它也不能给你证明两个定义是等价的。它就是在你的自由空间中给你穷举所有的
可能性。

也许我们可以这样理解：我们每个CPU都发出了一组内存操作，这组内存操作在每个观察
者看来，都有先有后，任何一种组合都可能。这是一个完全自由的集合变量空间。比如我
们观察前面提到的编程模型的up和us的关系，{(up, us)}可以是这个集合变量的一个解，
{(us, up)}也可以是这个集合的一个解。然后我们通过这个cat文件强制了一些条件，我
们能否把结果约束在{(up, us)}这一个解上？这就是Herd7帮我们穷举的东西。

注意了，我们这里说，Herd7只是给我们穷举一个可能性，它并不能输出约束之下的所有
解，因为这个计算量太大了，它就做不到。

所以，你只是给Herd7一个cat定义，说明你的约束，然后你给它一个检查条件（通常是一
段类似前面说的MP那样的一段代码），它告诉你你期望的那个目的（比如{(us, up)}一定
不是解的一部分）是否可以成立而已。

但用cat的格式说明一个规则，这种“标准化”的方法，有利于我们用一个统一的方法描述
问题。让实现CPU内存访问的芯片/硬件工程师和使用这个CPU的软件开发可以有统一的讨
论语言，这一点反而成了一件更重要的事情。即使我们不使用herd，cat也可以作为一种
“标准化”的讨论内存序问题的语言。

我们先看一个例子感受一下这个语言的而特征，然后随着这个解释更多的cat格式的概念。
比如下面是一个SC（前面提到的Sequential Consistency模型）的cat定义：::

  SC
  include "fences.cat"
  include "cos.cat"

  (* Atomic *)
  empty rmw & (fre;coe) as atom

  (* Sequential consistency *)
  show sm\id as si
  acyclic po | ((fr | rf | co);sm) as sc

两个头文件我们暂时不管，主要定义一些基本集合，我这里先解释一下这些基本集合的含
义：

* po：程序序，表示同一个CPU上的所有内存行为的序。
* fr: From Read，同位置读后写关系。
* rf: Read From，同位置写后读关系。
* co：Coherent Write，同位置的写后写关系。
* rmw：Read-Modify-Write，组成同一个原子指令的三个基本行为。
* fre：From-Read-External，跨CPU的fr。
* coe：Coherent Write External，跨CPU的co。
* sm: Same Memory，这个概念后面解释。
* id: 这表示所有事件自己和自己的关系（用来过滤事件用的内部常数）

然后我们看这个文件如何定义额外的约束。

首先，as xxxx这个语法表示某个约束的名字命名为xxxx。Herd完成穷举的后，如果找到
符合条件的例子会把这个例子的关系图输出来，类似这样：

.. figure:: _static/herd7-output.png

加上这个名字有助于可以在图上标记出这个关系，从而让你知道如何修正你的规则。后面
的show命令就是强制输出某个特定的关系。如果仅仅要看定义的规则，我们可以忽略它。

所以这个SC的定义仅仅定义了两个规则，一个叫atom，一个叫sc。

atom定义的是原子性规则，它说的是：rmw交fre;coe是一个空集。其中分号是“序列操作
符”，其实本质是复合函数。如果我们把关系集合看作是一个函数，每对关系就相当于函
数图像上的一个点，关系中的前一个元素就是定域域的输入，后一个元素就是值域的一个
元素。那么，两个关系集合的复合，就是把第一个集合的值域输入到第二个集合的定义域
中，得到第一个集合的输入和第二个集合的输出（中间有匹配不上的都放弃）。

比如我们计算{(a, b), (c, d)}:{(b, c), (d, e)}，输入a的时候，在第一个集合得到b，
用b作为输入在第二个集合中得到c，所以结果会得到(a, c)，如此类推，最终的结果就是
{(a, c), (c, e)}。

用序来理解就是：如果我们有两个序x和y，那么x;y就表示存在一个a-b-c这样的序，其
中a, b属于关系x，而b, c属于关系y。

所以这里fre;coe就表示下面CPU A的read_a和CPU C的write_a的关系：::

          CPU A                CPU B               CPU C
          read_a---\
                    \-(fre)--->write_a---\
                                          \-(coe)-->write_a
  
所有有这样的顺序关系的读写关系，都属于集合fre;coe。把这个集合交上rmw，rmw是一
条指令，表示同时做读-修改-写，这就表示上面CPU C的事情发生在CPU A上的那种情
况：::

          CPU A                CPU B
          read_a---\
                    \-(fre)--->write_a
                                 /
          write_a<------(coe)---/

所以这个意思就是说：如果A上的read_a和write_a两个事件属于同一个rwm指令，那么不
可能出现另一个CPU中的write_a，覆盖了a的值，还被A的write_a覆盖。这就是“rmw的原
子性”的定义。

我不知道读者是否注意到这一点：这个模型并不认为一个指令就是一个“内存事件”，这里
rmw本来只是一条指令，照理说就只产生一个事件，但实际上我们已经看到了，这有两个
事件。

所以这里的关键问题不在于几条指令，而在于我们有没有独立的行为可以单独关注到这个
事情。这个问题影响很多定义，比如一个原子的32位写操作，照理说应该是一个事件，它
也会被一个独立的读操作读到。但我们指令上也允许单独去读这个内存每个独立的字节。
为了说明这些每个独立的观察，我们也只能把这个原子操作定义成4个“内存事件”。如果
我们不需要推理那种情况，我们可以不分解这个定义，如果我们需要，那就只能分开，这
都会导致模型的不同。

所以，你不能认为模型就是“事实”，模型永远都是事实的“数字孪生”，你把什么东西放进
来讨论，你就只能模拟那些东西，它不是事实本身，也永远无法完全代表事实。

所以，其实就只有最后一个sc才是SC这个定义本身。为了理解这个定义，让我们先来理解
一下fr|rf|co的概念。fr表示一个地址上的值被一个写覆盖了。这听起来是个上帝视角，
没说是谁看见的。所以这样的定义存在，这个模型（herd7本身）已经承认内存至少在每
个独立可以观察的地址上是有“队列”的，这个fr指的就是在“内存”上，你再也读不到原来
的值了。

然后是rf，它表示一个读，读到了前一个写的内容。这是从发起这个读的观察者的眼中看
到的，如果(w, r)属于rf，那么r就是读到了w的值。至于它是通过cache读到的，还是通
过寄存器读到的，我们都不管。

最后是co，它表示coherent write，表示一个写，把前一个写覆盖了。和fr一样，这又是
一个上帝视角。这次让我画个图解释一下：

.. figure:: _static/co.svg

CPU有自己的Cache，当你要求访问内存，它当然可以选择写透Cache，一直写到内存上，
它也可以选择通知其他CPU，更新他们Cache的状态，让所有CPU都知道这个内存已经修改
了，再做下一个动作。这些动作的协议，称为Cache Coherency（CC）。如果你有实施CC
协议，无论你用的是什么方法，你在这个地址上总是形成一个序的。就是你的写，只要碰
到这个CC协议，你就会在CC这个接口上呈现一个顺序，让其他CPU在向这个CC接口请求数
据的时候，读到的数据就是符合这个序的。

但这不是必须的，如果我们不实现CC协议（就好像我们在很多CPU和设备之间通讯，要主
动更新Cache才能一些数据同步给设备），这一点并不成立。所以，你不要觉得Herd7给了
你所有的关于“关系”的自由度，其实它的语本身已经承认了很多东西了。

还有一个值得注意的点是：即使我们承认的CC协议，也不表示每个读写都会进入CC，因为
完全有可能在一个CPU上写了什么东西，在本CPU内部就被读走了，根本没有经过CC这个接
口。在后面的LKMM定义中，这种情形称为Forward。我们这里借用一些这个概念，也称为
Forward。

好了，下一个问题是sm是什么。这个其实我不知道，我几乎查不到关于这个概念的介绍，
无论是Herd7的文档还是它的源代码的注释。我还没有足够时间直接看着代码去还原这个
概念。我猜它的意思应该是Same Memory。但如果你注意前面我们作为例子用的那个输出，
我是故意把sm这个关系show出来的，它除了和自己的关系，其他同地址的关系基本都不认
为是sm。但我最后还是认为它是Same Memory（带条件的），后面我们解释。到现在为止，
我们理解的时候就当它是id好了。这样，“(fr|rf|co);sm”基本上可以简单理解为
“(rf|rf|co)”。

那么（rf|rf|co）这个东西又是什么意思呢？本质上它就是我们可以“观察到的所有顺序”。
请想想这个问题：当我们认为“事件A发生在事件B前面”，我们说的是“我观察到A对B的执
行效果的影响”。注意这个说法，我们不是“先看到A的效果，再看到B的效果”，因为在“关
系”的世界中是没有时间的。我们看到的“序”，都是关系。事件上一个事件发生在另一个
事件的后面的唯一观感是后一个事件的发生是以前一个事件的结果为前提的。fr表示我在
CC接口上看到本来可以读到的数据x现在变成y了，所以写发生在读的“后面”。这才构成了
序。所以，fr|rf|co就是所有可以观察的序。从这个角度来说，也许我们可以把sm看作
“（如果有的话）其他的关联影响”，就是如果后一个事件还引起了一个连锁反应，那么这
种观察也考虑在内。而读后读不是一种观察，你读了一个值，随后又读了一次。上帝视角
这在时间上有先后，但在观察上没有任何区别。所以这种关系不是“观察”的一部分。在很
多模型中，这种观察称为com（通讯）。

所以，po|com，如果是无环（acyclic）的（构成一个序），就是SC。

这和我们一般理解的SC很不一样是不是？回想前面把整个内存看作一个实体，把所有访问
都排到这个队列上的情景，那个队列上的顺序不是才是SC吗？

问题是，那个队列是个上帝视角，没法用com去确定事情发生了还是没有发生啊。再说了，
我们前面说，我们允许Forward的，那就有部分的com没有发生在那个队列上了，这也说不
通啊。

所以我们干脆换一个思路，我们把po和com放在一起，只要它构成一个序，那么我们已
经足够支持我们前面的编程模型需要的逻辑了。我们看一下这个MP的例子：

.. figure:: _static/acyclic_po_watch_init.svg

我们把up-s-w-us定义成一个“序”了，甚至不需要是全序或者偏序，那么编程上，我们就
能肯定us一定发生在up之后。为什么？既然这不是全序或者偏序，我们并没有可迁移性，
为什么可以认定us一定观察到up？

这要考量一下所有的可能性才能想清楚：us和up是同一个地址，如果us没有rf up，那程
序执行到最后，就只能是us rf初值，而up fr初值了。这样就会构成一副这样的图：

.. figure:: _static/acyclic_po_watch.svg

这很明显，这会产生一个up对us的反向观察，导致原来的路径成环。

把init看作是上下文1 po的一个最初步骤，这个条件还是成立的。所以其他po上的行为，
也有这样的性质。

通过这个感性的认识，我们可以发现，如果我们把有顺序要求的关系都放到一个集合中，
并要求它们“有序”，那么后面对前面没有覆盖的观察都是可以成立的，否则它必然被反向
观察，从而构成环，违反“有序”这个要求。

最后让我们总结一下：一个cat定义基本上就是集合和谓词定义，集合的主要运算符号我
在附录（\ :ref: `cat_op`\ ）中放了一个速查表，定义的语法和Ocaml是一样的，用let
var=xxx的形式表达，比如：::

  let ppo = po & [RR]

剩下的主要就是定义规则的“谓词”，它其实就三个谓词（未来根据需要是否要增加这就看
需要了）：

* acyclic
* irreflexive
* empty

有两个我们都介绍过，最后这个irreflexive（非反身映射）需要解释一下，这个概念完
全来自数学，表示id的任何元素都不属于所述集合（不存在自己到自己的关系），这个集
合就是irreflexive的。

序的理论有一个概念叫DAG（Directed Acyclic Graph，有向无环图）。这个有向，就是
irreflexive；无环，就是acyclic，如果两者都成立，就构成一个AGD。有标准算法可以
把AGD所有可能的全序穷举出来。Herd7有一个叫linearisation(E, r)的函数可以完成这
样的穷举。从这个角度来考虑序这个问题，Herd7的本质是通过规则定义一个的DAG，然后
穷举所有的满足这个DAG条件要求的全序（上帝视角的顺序），然后判断这些全序是否都
符合我们的期望。

谓词前可以加~表示取反。

更多的语法可以自己看手册，我这里介绍的应该足够支持看本文了。

litmus
^^^^^^

然后我们接着看litmus的例子：::

  X86 MP
  {x=0;y=0;}
   P0         | P1          ;
   MOV [x],$1 | MOV EAX,[y] ;
   MOV [y],$1 | MOV EBX,[x] ;
  exists (1:EAX=1 /\ 1:EBX=0)

这个语法很简单，基本上可以猜到含义：先给平台类型和名字，然后是设置变量初值，然
后是每个处理器上的汇编，最后是判定条件。

我想着重强调几个点：

1. 这个测试用例是平台相关的，因为每个平台的指令对模型的定义的解释是不同的。比
   如同一个测试，如果是ARM的，它是这样的：::

        AArch64 MP
        { 0:X1=x; 0:X3=y; 1:X1=y; 1:X3=x; }
         P0          | P1          ;
         MOV W0,#1   | LDR W0,[X1] ;
         STR W0,[X1] | LDR W2,[X3] ;
        
        exists (1:X0=1 /\ 1:X2=0)

   这在代码架构上很不好看，但开发者就是图这个方便，所以只能一个平台一个平台来
   单独理解，后面会看到，LKMM的测试用例也是按自己的语法来设计的。同一个cat定义，
   可以在模型上测试不同平台的litmus用例。

2. Herd可以基于模型推理litmus指定的条件是否成立，也可以直接生成平台相关代码让
   你真跑一下，看看那个平台硬件是不是真的符合条件（反复跑很多次尝试是否会遇到
   反例）。

3. 它的条件不一定是正向的，也可以是反向的，比如上面这个用例中，我们其实期望的
   是这种情况不会发生，但运行的时候你是希望发现有这种情况的时候告诉你。

我们可以这样运行这个模型：::

  herd7 -model sc.cat -nshow 10 -show prop -view gv mp.litmus

-model指定cat文件。-nshow表示最多显示多少个推理的结果。-show选择正向还是反向用
例的场景。-view选择显示引擎，这里用了gv格式。最后给定测试例就可以了。按上面的
sc.cat和x86的mp用例，我们显示正向用例（-show prop）结果就是没有。我们改成反向
用例（-show neg），我们就会看到gv显示的反例的输出了。

我们还可以试试修改一下sc.cat把sc规则删除掉，还是显示prop，我们会得到这样的输出：

.. figure:: _static/herd7-sc-mp1.png

这个图展示了我们前面说的理论：如果不对环作出限制，最终能达成us读到初值的条件就
是构成一个环。

实际上，这个litmus最终的目的还是让我们调试我们的模型定义，这里的重点还是优化
cat的定义。在本文中，我们更关心的是用cat语法来描述我们对硬件设计的要求。


内存序的物理模型
----------------

前面介绍了数学模型，数学模型是边界而已，我们没法按着它来设计总线和CPU的，数学
上定义出来的原则，可以用来约束物理模型，但物理模型必然会引入额外的约束。反过来，
物理上有额外的约束，但数学模型不使用这个约束，其实也给软件带来来浪费。因为这个
地方本来没有自由度的，非要给一个自由度，软件就要加分支去处理，但这个分支从来不
进去，变成了浪费资源。

所以，总结一下物理实现上的抽象模型，有助于我们优化数学模型的。

关于po
~~~~~~

在这之前，让我们深入探讨一下po。po是个天然的概念，因为我们一开始定义指令的时候，
是隐藏了这个概念在里面的。我们认为CPU的状态在一个一个的指令驱动下发生改变，从
而形成一个“序列”或者“线程”的。这里天然就是在描述一个“全序”。

但前面读者已经看到了，实际上我们如果需要深入探讨各种关系的时候，po的事件集合就
不能是指令。

甚至现在有些平台在把取指，Page Walk的访存行为也放到内存模型中来讨论，这个po的
基本事件集合就变得非常复杂了，我们也很难直接认为它是一个全序了。

所以，我们以前可以很自然把po看作是一个全序，其实现在这个事情已经变得非常困难了。
我们只能认为po是一个偏序，甚至有时只能把它作为一个“序”，无法规定它的全序。像取
指和Page Walk这种很难预期的行为，常常是没有确定的先后关系的，比如，取指，往往
是一次取多条指令，然后同时发出执行的，这种情况你不能说序列是“取第一条指令，根
据第一条指令的要求访问内存，取第二条指令……”，你也不能确定地说，一定是“取8条指
令，执行8条指令，然后再取八条指令……”，你甚至不能说“必然在取指后才执行某条指令”，
因为这个取指行为完全有可能被缓存到CPU内部，导致根本不产生取指操作。

所以，如果可能，取指这个行为我们根本就不定义在一般的模型中，我们把它作为一个独
立的模型来定义，这样才能避免多余的复杂度。（但显然，某些平台不是这样做的。这只
能说，各有选择了。）

但由于po不是承诺的序，所以，其实我们非要把po定义为一个全序，并没有什么不可以的。
把po定义为一个全序的好处是，我们再定义其他的序的时候，就可以以这个序为基础，这
能简化模型。

这个问题，现在很多定义都是模糊其词的。po必须被认为是一个全序，因为没有这个承诺，
我们没法说清楚CPU对“线程”的承诺，但很多地方我们就只能简单认为它是一个序，而没
法承诺它一定是全序。

对此，我进行一个折中，我们还是尽量让po接近一个全序，但这个全序的其中一段是无序
的，类似这样：::

  A -> B (a, b, c) -> C -> D (d, e, f)

ABCD是个全序，但B可以有多个子事件组成，这几个事件每个可以取代B构成这个全序，但
a, b, c之间是不一定有序的。比如B是一个SIMD指令，同时操作多条Lane，哪条Lane算在
前面？这不确定。但我们能肯定的是，SIMD指令前面的指令在任何一条Lane的前面，而
SIMD后面的指令在任何一条Lane的后面。在本文中，我把(a, b, c)这个集合称为B的无序
替代。如果a, b, c是有序的，我称为它是针对这个全序的有序替代。::

  B对po的无序替代：A ->a/b/c -> C -> D
  B对po的有序替代：A -> a -> b -> c -> C -> D

CC接口
~~~~~~

对CPU来说，内存操作是个慢速行为。在CPU的流水线中，一条指令可能需要经过取指，解
码，执行，访存，回写等多个阶段，每个阶段不过1到数个时钟周期不等，由于流水线的
作用，某条指令执行后面阶段的时候，执行前面阶段的硬件已经在执行下一条指令了。所
以综合起来，一条指令的执行时间不过一个或者几个时钟周期（通常就是1个），但一次
内存访问就要上百个时钟周期。所以CPU有足够的理由缓存部分数据在CPU内部，一旦这个
缓存存在了，就会出现我们前面提到的Forward问题：数据可能不用经过CC接口就在内部
消化了。

我们当然可以说如果它修改过这个数据，最终总要更新到CC接口上的。但别忘了，事情可
以这样发生的：在CPU内部写了一个值a，然后它被读走，然后CPU内部再写了一个值b，之
后b被写出去，那么a写这个行为就在内部被消化了，在CC接口上从来没有发生过。

这是CPU内部的情况，我们再看看CC接口上的行为。首先，我们忽略多层Cache的问题，因
为本质上，CPU一层看到的CC接口，已经代表内存的态度了，CC接口的下一层如果还有一
层Cache，那么是这个上层的CC接口通过CC接口再去为下一层的一致性负责，对CPU来说，
它只考虑CC接口的承诺就行了，下一层都由这个地一层的接口代表了，对运行在CPU里面
的程序来说，下一层的逻辑是可以忽略的：

.. figure:: _static/cc_if.svg

所以，我们更关心的是第一层的CC协议到底怎么承诺这个序的。这又分成这个CC接口的内
部和外部两个部分了。

我们先看内部。首先几乎所有的CPU都要保证po构成的逻辑必须在单个CPU上是一致的，所
以只有一个CPU的话，po的序就是逻辑判断可以依靠的序，这一点算是种向前兼容吧，至
少现在还没有人会推翻这个逻辑。

关键就在于，这个离开不一定就是按po的顺序离开的。比如对于TSO（x86采用的内存序模
型），它称为Total Store Order，表示“写是一个全序”。它在CPU内部放了一个队列，所
有的写都必须排队然后才到CC接口上，所以写出去是有序的，而读的，如果读的内容还在
写队列中，那么就从写队列读，如果不在，那就直接出去了。这变成两个序了：

.. figure:: _static/tso-queue.svg

对这个模型做推理，结论就是它在对外上，写写，写读，读读都是保序的，就是读写不是
保序的。这是它和SC模型的主要区别，后者是全保序的。

作为对比，我们还有ARM，RISCV等采用的WMO方案，这个干脆什么出去的顺序都不保证，
只要指令没有强制要求，他们都直接出去了。所有的保序都是针对CPU本地的，出去的顺
序这些模型都不保证。如果这些平台需要保证确切的顺序，就需要使用特定的指令去控制，
这成为Memory Barrier。在WMO方案上，Memroy Barrier显得特别重要。

.. warning:: 需要说明的是，WMO不是一个明确定义的模型，它们在不同平台上是不同的。
   而且，这些不同的实现也不是完全没有队列，但由于队列的使用策略不同，也会形成
   不同的顺序模型。

然后我们看外部，根据《A Primer on Memory Consistency and Cache Coherency (2nd
Edition）》的定义，CC接口大致可以分成两种：

1. Consistency-Agnostic Coherence，CAC，一致性不可知CC
2. Consistency-Directed Coherence，CDC，一致性指定CC

前者通常用在CPU上，表示CPU不知道CC协议具体是怎么同步的，所以做写操作必须等所有
同步对象承认已经同步成功了，才认为写成功了。后者通常用在GPU上，表示GPU知道CC协
议的具体行为，它可以根据需要决定是否等待写成功返回。

但这是个模糊地带，因为现在CPU也集成了不少向量计算单元，根据需要这个接口也是有
可能发生变化的。

所以，如果我们确定CC接口使用的是CAC协议，我们可以认为离开CPU内部队列的请求，在
CC接口开始是有序的。但如果无法确定，我们并不能保证这一点，我们只能退一步，承认
对于同一个地址，这些行为的有序的。这一点现在看来是几乎所有CPU都承认的。

这里提到两种CC接口，让我们注意到一个事实：指令只能管到CC接口，如果指令一开始不
等待CC接口完成（比如一个写操作，发出去后CPU决定不等待写的完成消息，那么之后CPU
就完全管不了这个写操作是否完成了）。

现在很多CC协议会衍生到设备上，比如海思的鲲鹏处理器，所有系统设备也工作在CC接口
上，但它的CC协议地位和CPU是不一样的，总线不一定认为设备上有Cache，它可能仅仅是
更改某个页目录的状态，保证设备需要读内存的时候从正确的地方去读。所以，我们这里
的讨论，特别是性能相关的，不能简单应用到设备上，那些必须具体问题具体分析。

预测执行导致的乱序
~~~~~~~~~~~~~~~~~~

除了队列可以导致乱序，预测执行也会造成乱序。下面这个例子来自LKMM，但它是硬件实
现导致的：

.. code-block:: c

  q = READ_ONCE(a);
  if (q)
    p = READ_ONCE(b);

如果按po，这个顺序应该是先读a，然后读b。但CPU可以预测执行，它可以发射a的操作，
在a返回之前，它可以先尝试预测执行读b的操作，这个b就可以先发射出去了。等a的结果
返回了，如果正好就不等于0，那说明预测正确了，读b的操作就直接生效。这样在CC接口
上看到的就是先发射了b，然后才发射a。

所以你不能认为po上有了依赖，就一定能保证发射到CC接口上也是有序的。不能正确认识
一点，就可能导致程序写错，因为和你通讯的程序可能先更新了a，然后才更新b的，你提
前读了b，然后才去读a，那个b就是一个旧版本的内容了。你必须在if语句前放一个rmb()
才能保证后面的读访问不能发射出去。

这个问题就算是一些TSO平台都不一定可以避免，因为这两个都是读操作，不需要经过写
队列的。

小结
~~~~

让我们总结一下这个模型：我们认为一个SMP系统由一组CPU组成，通过CC接口连在一起，
CPU内部按po维持语义逻辑，但不一定按po的顺序把内存请求发送的CC接口上，也不一定
把所有的请求都发送到CC接口上。如果请求发送到CC接口上了，同一个地址的请求会被保
序，但如果不是同一个地址，就不一定会保序。

其他的，都是平台相关的特殊特性。

LKMM
====

介绍
----

我们前面说的内存序模型都是基于硬件的，LKMM是把一样的理论用于Linux Kernel编程接
口，这是一个基于软件的模型。

软件模型会叠加很多软件的要素，一个最基本的，是编译器的影响。比如我们一开始提到
的例子：

.. code-block:: c

  *((int *)0x12340000) = 1;
  *((int *)0x12340000) = 2;
  output(*((int *)0x12340000));

编译器完全没有必要执行前面两条指令（执行一条就够了），如果不指定地址，而是一个
就在这个上下文上有效的变量，一条都不会执行。只要output(2)就行了。

所以LKMM要专门区分Plain和Marked两种访问，所谓Plain的访问，就是像上面这样，直接
写访问的代码，这种是不一定真的产生内存访问指令的。而Marked访问是用READ_ONCE()
或者WRITE_ONCE()这样的接口（包括基于这种接口实现的其他接口，比如锁、RCU操作、
smp_store_release()/smp_load_acquired()等）强制真的产生真正的指令。无论硬件按
什么顺序实际访问过去，但至少指令的真的。

但就算加上Marked访问，也不能保证我们的所有期望都可以成立。下面是另一个例子：

.. code-block:: c

  q = READ_ONCE(a);
  if (q) {
        WRITE_ONCE(b, 1);
  	do_something();
  } else {
        WRITE_ONCE(b, 1);
  	do_something_else();
  }

这个类似前面提过的预测执行问题，我们期望读a以后再写b，但因为预测执行的问题存在，
这可能不符合期望。但及时没有这个预测执行，这个也是没有保证的，因为编译器有可能
会这样优化它（两个分支中有重复代码）：

.. code-block:: c

  q = READ_ONCE(a);
  WRITE_ONCE(b, 1);  /* BUG: No ordering vs. load from a!!! */
  if (q)
  	do_something();
  else
  	do_something_else();

一旦代码被编译器改成这样了，不需要预测执行，就算是TSO也不要求读后写要保序的，
最终的结果就可能是先写出去了，这样读进来的值可能就不是你期望的值了。

还有些问题是，语言本身就没有规定任何序。比如如下程序：

.. code-block:: c

  a = b + c;

这个加法，C语言并没有规定先加载b还是c。

软件模型的另一个问题是它是跨平台的，同一段代码我们是期望在不同的硬件平台上都要
满足期望的，这即使在一个平台上测试成功，也不能保证在所有平台上都是成功的。所以
研究和定义LKMM显得尤其重要，否则我们无法知道硬件平台（包括编译器）需要满足什么
要求才能保证Linux Kernel可以正常工作。而写这种类型的代码，也必须严格按LKMM的模
型来写程序，否则都是不能肯定代码是具有持续性的。

LKMM规则
--------

LKMM定义在内核源代码树的tools/memory-model/linux-linux.cat中，它主要包含这些规
则（我忽略了原子性方面的规则，因为我们这里重点讨论序的问题）：::

  acyclic po-loc | com as coherence
  acyclic hb as happens-before
  acyclic pb as propagation
  irreflexive rb as rcu
  acyclic xb as executes-before  （* 这个是意图上的，不是实际定义的 *)
  
我们重点理解一下这些规则的含义。

coherence
~~~~~~~~~

com是观察，这个我们前面解释过了。po-loc是同地址的po访问。所以，这个coherence表
达的是同地址的po（后面我们就直接叫po-loc吧），被观察的时候是有序的。这很像之前
解释过的SC模式，只是它限制了是同一个地址。

这个规则，我们基本可以认为是一个弱化（弱很多）版本的SC。

happens-before
~~~~~~~~~~~~~~

这是LKMM最基本的可以依赖的序：::

  let hb = [Marked] ; (ppo | rfe | ((prop \ id) & int)) ; [Marked]

这个规则定义两个Marked的内存行为之间的序，只要两者被ppo，rfe或者内部prop连接，
就在范围内。

我们看看这些基础元素的含义：

首先是ppo，preseved po。这表示在po上明确说对外保序的行为。比如依赖明确的
barrior行为等等。

依赖主要指这三种：

* data：后一个内存访问需要使用前一个内存访问的结果。比如a=b+3。
* ctrl：执行分支上的内存访问需要前一个内存访问结果。比如if(a)b+=3。
* addr：后一个访问序要前一个访问的结果作为地址。比如b[a]=3。

这里要澄清一点：前面提到的预测执行的例子并不违反这里的hb涉及的ppo定义。因为读
了一个值，然后基于这个值来做跳转，这个序就算预测执行也是被保证的，但在这之后再
做一个操作，那个操作是否还保序，是没有承诺的。

所以这里一定避免把保序的边，当作一种barrior，觉得它维持了所有的顺序，它仅仅是
在保证所有这样的边，不会造成自环，只有有一环扣不上，它就有可能逃过控制。这在判
断的时候需要非常小心。

我们还要注意这个依赖得是真依赖，这个在使用宏的时候很容易搞错，比如前面的例
子：::

  b[GET(a)]=3;

这种情况b依赖a对吧？但某些#ifdef分支中，这个GET(a)固定返回0呢？你看着代码觉得
这里有个依赖，其实它没有。

另外，我们这里用了barrior这个名字，指的是memroy barrior，不是barrior()函数。前
者是指smb_mb()，smb_rmb()，smp_store_release()这样的函数，这会产生真正的内存屏
障指令，而barrior()是“编译器屏障”，它只是给编译器提示说这个位置发生了内存修改，
如果后面要访问某个变量，不要认为变量的值已经加载过，要重新加载进来。它不产生内
存屏障指令的。我们这里讨论的所有屏障，都不包含这种屏障。所以后面我们也尽量不用
barrier这个词，我们把这些行为称为fence。

fence很依赖内存序的物理模型，特别是CC接口到底是CAC的还是CDC的。LKMM把这个物理
模型称为操作模型（Optional Model），它的定义既不完全是CAC的，也不完全是CDC的。

先列一下它支持的fence：

* synchronize_rcu()，强制这个fence之前的内存操作必然比fence之后的内存操作先发
  射到CC接口上。

  同时，所有在这个fence之前传播到本CPU的写，必须在本CPU这个fence之后的所有内存
  访问之前传播给所有的CPU。也就是这个CPU在fence之后看到的更新，其他CPU都已经看
  到了。

* smb_mb()，和前者统称强fence，有前者一样的功能。

  同时保证之前的写全部传播到其他CPU上，然后才会轮到之后的写。这不完全是CAC的逻
  辑（仅包括写），但很类似。它保证其他CPU如果看到fence后面的写，一定看到前面的
  写。

* smb_rmb()，强制这个fence之前的读内存操作总比之后的读操作先发送到CC接口上。没
  有传播上的额外保证。

* smb_wmb()，强制这个fence之前的写内存操作总比之后的写操作先发送到CC接口上。没
  有传播上的额外保证。

* smb_store_release()，这是一个写，它保证这个写必然后于之前的内存操作发射到CC
  接口上。（这个和一般学术的定义不同，一般定义是仅仅fence之前的写操作。）

  同时（关键是这个同时），如果这个fence执行的之前，有任何CPU的写操作传播到本
  CPU（包括本CPU之前的写），那么这些所有的写，都需要先于这个写本身传播到其他
  CPU。换句话说，如果本CPU在这个fence（兼写入）之后看到了别人的更新，别人就必
  须看到它之前的所有更新。

* smb_load_acquire()，这是一个读，它保证这个读必然先于之后的内存操作发射到CC接
  口上。（同上，这也和一般学术定义不同，一般定义是仅仅fence之后的读操作。）
  
  没有传播上的额外保证（release和acquire是成对完成MP一类的任务用的，release已
  经保证acquire的目的达到了）。

这个定义既不是CAC也不是CDC。只能猜，LKMM在找一个所有平台都能实现的定义，最后找
出来就是现在这样的。

todo：先存盘，后面接着写。


rfe这个之前解释过了，我们必须补充一下，这里特别强调是rfe而不是rf，是因为只有跨
了CPU（或者经过了调度也行），这个行为才会经过CI结构发生传播。rfe是我们编程依赖
的中心，因为我们总是根据某个值等于某个写了，才决定其他其他值应该处于什么状态，
所以判断都是依赖rfe这个结构。rfi（rf internal，po内的rf）本来也不是多线程通讯
考虑的问题，所以这里有这个rfe作为整个序的一环，其实也不影响使用。



propagation
~~~~~~~~~~~


rcu
~~~


executes-before
~~~~~~~~~~~~~~~


附录
====

.. _`cat_op`:

cat主要操作符的解释
-------------------

* \|, &, \，;表示并集，交集，差集和序列。

* {}表示空集，++是元素加到集合中。

* r+和r\*分别表示r的迁移闭包和迁移反射闭包。0表示空集。r^-1表示r的反射。r?表示
  r加上iden。（注意了，herd中这个反射包括所有事件的全集，但我看数学上的定义似
  乎只包括r包含的元素本身。)

  所谓迁移反射的定义是：r\* =r+ | id

* R*W表示R和W的笛卡尔乘积。

* [E]=E*E & iden

从序列的角度来理解，上面这个符号体系其实有点像正则表达式。

如果我们有一个序列r1;r2;r3，这个序列定义了一个这样的关系：::

  a--(r1)-->b--(r2)-->c--(r3)-->d

这样的序列中的a->d关系。

如果我们加上一个+号，变成这样：r1;r2+;r3，这表示r2这个关系还需
要存在，但可以有多个：::

  a--(r1)-->b--(r2)-->c--(r3)-->d
  a--(r1)-->b--(r2)-->c--(r2)-->c1--(r3)-->d
  a--(r1)-->b--(r2)-->c--(r2)-->c1--(r2)-->c2--(r3)-->d

这些情况的序列，都符合我们定义的a->d关系。

如果我们加上一个\*号，变成：r1;r2*;r3，这表示r2这个关系可以有任意多个，那么除
了前面的，这个也符合条件：::

  a--(r1)-->b--(r3)-->d

如果我们家一个?号，变成：r1;r2?;r3，这表示r2可选，那么这两种情况都符合定义：::

  a--(r1)-->b--(r3)-->d
  a--(r1)-->b--(r2)-->c--(r3)-->d

这和正则表达式的通配符是一样的。而[]操作符可以用于过滤，比如[R];r1;r2;r3;[W]，
如果R表示所有的读，W表示所有的写。那么这个表示在r1;r2;r3定义的序列中，前后的两
个操作必须一个是读，一个是写。

如果我们写成这样：r1;[R];r2，这表示连接r1和r2的那个操作必须是个读。对于下面这
种情况：::

  a--(r1)-->b--(r2)-->d

它要求b必须是个读。理解这一点，就比较容易读懂LKMM的各种定义了。
